---
title: "Full Stack Alignment"
pubDate: 2024-03-05
description: Feb reflection, website status updates, flower computers again
foregroundColor: "lab(44 40 88)"
backgroundColor: "lab(90 11 33)"
foregroundColorDark: "lab(33 50 99)"
backgroundColorDark: "lab(1 20 90)"
---

- Contents

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c0657f85-93c3-40ed-9746-a796013f91f7/Untitled.png)

# Full Stack Alignment

Our approach to ‚Äúaligning ML with human values‚Äù differs from other approaches in the field.

1. First, it depends on **values-articulate people** ‚Äî¬†people who understand their Sources of Meaning ([Making Values Concrete](https://www.notion.so/Making-Values-Concrete-7c3903dbb6424aa39987d9363a90965f?pvs=21)), and can critically assess whether systems they're part of support them to live meaningfully. (Our [Values Elicitation Bot](https://www.notion.so/Values-Elicitation-Bot-f1d983c9bb28403ca5b51f8b65e7bbaa?pvs=21) and **Events** help people become meaning-articulate.)
2. Second, it‚Äôs based on a **particular conception of human values** ‚Äî ‚ÄúValues as Sources of Meaning‚Äù.¬†In this approach, values are abstracted patterns of agency that give meaning to people‚Äôs lives, and can be surfaced by examining the details of meaningful experiences. This concrete, testable conception of human values allows **meaning-articulate people** to evaluate the alignment of ML models and institutions for themselves, enabling ‚Äúfull stack‚Äù alignment.

These two features allows us run a rapid playbook to create alignment at many levels of society.

# Current ML Research

We coordinate research across multiple labs to create ML models that:

- (a) Understand and support what's meaningful to individual users (Meaning Assistants)
- (b) Map the sources of meaning behind broad social activities (like science, democracy, education, leadership, and love) and run Values-Explicit Deliberation processes to guide their own execution.
- (c) _eventually_ develop their own values and sources of meaning (just as humans do) when they enter new positions of responsibility. (Artificial Super-Wisdom)

<aside>
üßëüèº‚Äçüî¨ Our [Evaluation Framework (Project)](https://www.notion.so/Evaluation-Framework-Project-cc90abf9a2c445c4b8da44b61e9c81f8?pvs=21) is a series of papers co-written by researchers at DeepMind, Anthropic, OpenAI which lay out evaluation criteria for Meaning Assistants, Democratic AI, and Artificial Superwisdom.

</aside>

<aside>
‚è≤Ô∏è **Personelle**

- [Ryan Lowe](https://scholar.google.ca/citations?user=iRgYMuEAAAAJ&hl=en) + a small team at OpenAI
- [Ivan Vendrov](https://scholar.google.com/citations?user=k8TR3FQAAAAJ&hl=en) at Anthropic
- [Joel Lehman](https://scholar.google.com/citations?user=GcvxHWQAAAAJ&hl=en), an ex-OpenAI architecture person
- [Vlad Mikulik](https://scholar.google.com/citations?user=o42aK1UAAAAJ&hl=en) at DeepMind
- Oliver Klingefjord at AI Objectives
- [Luke Thorburn](https://scholar.google.com/citations?user=VNF3f_QAAAAJ&hl=en) at KCL
</aside>

## Meaning Assistants & Recommenders

We want recommenders & assistants **recommenders and assistants that understand what's meaningful to us** and work to help us with _that_ (rather than just driving engagement). Which honor what is noble and great in human life. Economists have discussed how this should be characterized for decades. It‚Äôs the main question of welfare economics, where Amartya Sen and others converged on a conception of [human values as distinct from human preferences](https://docs.google.com/document/d/1U1fJm-f_YtA_hRwLChKtC4Xw79zUWGDAeABk9LhkgEs/edit).

This is related to the question of what‚Äôs intrinsically rather than instrumentally valuable to a person. First, because that‚Äôs what we‚Äôre really after. Second, because it‚Äôs easy for third parties to manipulate what‚Äôs _instrumentally_ valuable (e.g., it's instrumentally valuable to be an influencer on instagram, but this is less about the user, more about instagram).

- Why align with meaning?
  **Alignment people generally try to align AI to something we tell _it_ to do**. But it's easier to align to something that we want to do _ourselves_. If we imagine the goal is to get something done, the AI will end up as a kind of accelerationist, trying to do as much of that as possible. That‚Äôs a problem. Instead, we can tell the AI what _we_ want to do (how we want to live), _ourselves._ This makes alignment easier.
  The next question, then, is what is it that is honorable and noble in human life, that the AI can assist us with. Economists have been discussing this question‚Äîthe main question of welfare economics‚Äîfor decades. Amartya Sen‚Äôs work is central here. Sen and others converged on a conception of [human values as distinct from human preferences](https://docs.google.com/document/d/1U1fJm-f_YtA_hRwLChKtC4Xw79zUWGDAeABk9LhkgEs/edit).
  I have taken this further, developing a representation of values (as sources of meaning) which is robust to various forms of manipulation. (See [[Intro] Scale and Space Decay](https://www.notion.so/Intro-Scale-and-Space-Decay-d269b5b43d7d48889c8c1ec594381805?pvs=21), and [Operationalizing Metaethics](https://www.notion.so/Operationalizing-Metaethics-2919bbd6a882487fb3083029feaceffa?pvs=21))

One possibility is to use [a representation of values](https://github.com/jxe/vpm/blob/master/vpm.pdf) ([Chapter 4. Values Cards](https://www.notion.so/Chapter-4-Values-Cards-dfb857c6eb834b9c90629a6627459d23?pvs=21)) which hews towards the intrinsic, and is robust to various forms of manipulation. (See [[Intro] Scale and Space Decay](https://www.notion.so/Intro-Scale-and-Space-Decay-d269b5b43d7d48889c8c1ec594381805?pvs=21), and [Operationalizing Metaethics](https://www.notion.so/Operationalizing-Metaethics-2919bbd6a882487fb3083029feaceffa?pvs=21))

## Democratic AI + Values-Explicit Deliberation

We want AI agents that, in interacting with a population, govern themselves via the democratic wisdom of that population, operating using the best values they can surface from the populations they serve.

This likely involves (1) mapping the meanings that make society work, and (2) running values-explicit deliberation processes.

1. **Which Meanings Make Societies Work?** (Big Data Virtue Ethics) The goal here¬†is to map the {virtue, environment} pairs which make human life meaningful and workable. We can do this, for instance, with methods for assisted introspection about values (such as with our LLM-based chatbot), or by automatically extracting values from influential texts (see this tweet).
2. In **Values-Explicit Deliberation Processes**, the focus is first on what's important; later on the choice itself. Easier to agree on what's important at the level of meaningful living: identical values are coalesced, ideological commitments eliminated by Values Elicitation Chatbot and Values Cards.

   `Project page` [Values-Explicit Deliberation](https://www.notion.so/Values-Explicit-Deliberation-8c0cdd13064d44b68d0d3b8d97e66e94?pvs=21)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4bb05f3f-a2f2-410a-8901-850be7b20cb0/Untitled.png)

<aside>
üê¶ **Special personnel**

- Joe Edelman
- Lily Lamboy (Rebuilding Meaning, Political Theory)
- Oliver Klingefjord at AI Objectives (Deliberation Tech)
- [Luke Thorburn](https://scholar.google.com/citations?user=VNF3f_QAAAAJ&hl=en) at KCL (Deliberation)
</aside>

## Artificial Super-Wisdom

Finally, multi-lab work on **Machines Which Progress Morally** will produce ML assistants which recognize situations of moral responsibility and select new moral values.

In humans,¬†[moral emotions guide this process](https://textbook.sfsd.io/cbf6b01d256a4c908d2aa3bb1f470641). Human values evolve as we enter positions of responsibility and gain new knowledge of the world. We think we train AI agents to do this by mimicking [ Human Moral Learning](https://www.notion.so/Human-Moral-Learning-c502faf14a0c4cf1be786cc506be271a?pvs=21).

We're starting that investigation with multi-lab paper detailing an [Evaluation Framework (Project)](https://www.notion.so/Evaluation-Framework-Project-cc90abf9a2c445c4b8da44b61e9c81f8?pvs=21) for evaluating LLMs that gain ‚Äúwisdom‚Äù (moral values) as they encounters new situations of responsibility.

`Project page` [Evaluation Framework (Project)](https://www.notion.so/Evaluation-Framework-Project-cc90abf9a2c445c4b8da44b61e9c81f8?pvs=21)

# Future Research

Finally, we want to align global incentives so market and state actors choose AGW.

Currently, the incentive structure of markets and organizations aren‚Äôt aligned with the general interest. Market actors (for instance, hedge funds) already use ML models to accelerate their work in those unaligned directions. We must presume they‚Äôll continue to do so, even if aligned ML models were available: a hedge fund or military actor will prefer an unaligned AI, because it will make more money and provide more geopolitical advantage if it doesn‚Äôt have this sense of moral responsibility and values.

The alignment problem, then, includes ensuring market and state actors will make the right choice. This sounds pretty hard.

## Meaning Commonses

**Here‚Äôs one way to think of this**: the global competitive landscape currently includes large actors following monetary incentives, and other large actors following political incentives. Neither of these can make consistent, legible, values-based choices the way human beings can. What if we could introduce, into this competitive landscape, a third kind of large actor‚Äîone that _can_ make consistent, legible, values-based choices, and learn from the best values of the people that make it up?

Here‚Äôs the idea: unaligned market actors (like hedge funds) and political actors (like nation states) are outcompeted via introduction of a third type of large institution which uses **Values-Explicit Deliberation** (see above) and **Meaning Allocators** (below) internally and Values-aware Contracts and Treaties externally.

We suppose that, if such actors become more common in the competitive landscape, they could take significant power away from unaligned actors like hedge funds and conventional nation states.

- **Meaning allocators** are a way to allocate and finance club goods, based on what's meaningful to people in the club. As resources are added, more people's Sources of Meaning are well-supported.
- **Other meaning-respecting social structures**. Meaning commonses will likely also mean inventing new mechanisms and structures throughout the [social stack](https://www.notion.so/Social-Stack-a9cbec38007040aeb1a1162fd3b494b3?pvs=21) (e.g., new kinds of organizations, contracts, voting structures, insurance and auditing structures, funding, land ownership patterns, various kinds of markets, etc), which better support meaningful lives. Such new institutions may process new information‚Äîfor instance, about each player‚Äôs [_sources of meaning_](https://www.notion.so/Making-Values-Concrete-7c3903dbb6424aa39987d9363a90965f?pvs=21)‚Äîmuch as markets are understood to take in preferences and output allocations.
  As part of this process, we will also evaluating existing institutions (like org structures, or democratic deliberation structures) in a new way: in terms of how well they support the relevant sources of meaning for participants. We can do this with surveys about meaning, with other kinds of data, and also with arguments.
  (And we will likely need to develop a [Theory of Values-Based Choice](https://www.notion.so/Theory-of-Values-Based-Choice-6d92889552f14d8aa61168bf4cd0b79c?pvs=21).)
